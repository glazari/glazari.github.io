<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8" />
<title>Testing Pyspark</title>


  



<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link
  rel="alternate"
  type="application/rss+xml"
  href="https://blog.guilhermelazari.dev/index.xml"
  title="Guilherme LÃ¡zari"
/>

<link rel="stylesheet" href="https://blog.guilhermelazari.dev/fontawesome/css/all.min.css" />

<link
  id="dark-mode-theme"
  rel="stylesheet"
  href="https://blog.guilhermelazari.dev/css/dark.css"
/>

<script>
  var darkTheme = document.getElementById('dark-mode-theme')
  var storedTheme = localStorage.getItem('dark-mode-storage')
  if (storedTheme === 'dark') {
    darkTheme.disabled = false
  } else if (storedTheme === 'light') {
    darkTheme.disabled = true
  }
</script>

<script src="https://blog.guilhermelazari.dev/js/bundle.js"></script>
<script src="https://blog.guilhermelazari.dev/js/instantpage.min.js" type="module" defer></script>
<meta name="generator" content="Hugo 0.101.0" />
  </head>
  <body>
    
  




  <header>
    <nav class="navbar">
  <div class="nav">
    

    <ul class="nav-links">
      
        
          <li>
            <a href="/" id="Home"
              ><em class="fas fa-home fa-lg"></em
            ></a>
          </li>
          
      
        
          <li>
            <a href="/about/" id="About"
              ><em class="fas fa-user fa-lg"></em
            ></a>
          </li>
          
      
        
          <li>
            <a href="/tags" id="Tags"
              ><em class="fas fa-tag fa-lg"></em
            ></a>
          </li>
          
      
    </ul>
  </div>
</nav>

    <div class="intro-header">
      <div class="container">
        <div class="post-heading">
          
            <h1>
              Testing Pyspark
            </h1>
          
          
            <span class="meta-post">
  <em class="fa fa-calendar-alt"></em
  >&nbsp;Nov 29, 2020
  
</span>

          
        </div>
      </div>
    </div>
  </header>
  

    
  <div class="container" role="main">
    <article class="article" class="blog-post">
      
  <p>We all know unit testing our code is a good idea. But there is some kind of inertia
that keeps most of us from doing it. Part of this inertia, I believe is not knowing
how to start. So I am sharing how I write tests for pyspark in the hopes that
it will give you the push you need.</p>
<h1 id="testing-in-python">Testing in python</h1>
<p>First things first, if you&rsquo;ve never written a test for a normal python function
we need to get that out of the way first. Here is a minimal example:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">add</span>(x: int, y: int) <span style="color:#f92672">-&gt;</span> int:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> x <span style="color:#f92672">+</span> y
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">test_add</span>():
</span></span><span style="display:flex;"><span>    inputs <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#34;x&#34;</span>: <span style="color:#ae81ff">3</span>, <span style="color:#e6db74">&#34;y&#34;</span>: <span style="color:#ae81ff">4</span>}
</span></span><span style="display:flex;"><span>    expected <span style="color:#f92672">=</span> <span style="color:#ae81ff">7</span>
</span></span><span style="display:flex;"><span>    got <span style="color:#f92672">=</span> add(<span style="color:#f92672">**</span>inputs)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">assert</span> got <span style="color:#f92672">==</span> expected
</span></span></code></pre></div><p>Save to <code>my_test.py</code> then run <code>pytest my_test.py</code>, there you go! Now Back to
pyspark.</p>
<h1 id="basic-script-pattern">Basic Script pattern</h1>
<p>Let&rsquo;s see an example pyspark script for generating a denormalized sales table,
that is, we&rsquo;ll add some user information to each transaction line.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.sql <span style="color:#f92672">import</span> SparkSession
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>spark <span style="color:#f92672">=</span> SparkSession<span style="color:#f92672">.</span>builder<span style="color:#f92672">.</span>master(<span style="color:#e6db74">&#34;local&#34;</span>)<span style="color:#f92672">.</span>getOrCreate()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>sales <span style="color:#f92672">=</span> spark<span style="color:#f92672">.</span>read<span style="color:#f92672">.</span>csv(<span style="color:#e6db74">&#34;./sales.csv&#34;</span>, sep<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">&#34;</span>, header<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>users <span style="color:#f92672">=</span> spark<span style="color:#f92672">.</span>read<span style="color:#f92672">.</span>csv(<span style="color:#e6db74">&#34;./user.csv&#34;</span>, sep<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">&#34;</span>, header<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>df <span style="color:#f92672">=</span> sales<span style="color:#f92672">.</span>join(users, on<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;user_id&#34;</span>], how<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;left&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>df<span style="color:#f92672">.</span>write<span style="color:#f92672">.</span>mode(<span style="color:#e6db74">&#34;overwrite&#34;</span>)<span style="color:#f92672">.</span>csv(<span style="color:#e6db74">&#34;./full_sales.csv&#34;</span>, sep<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">&#34;</span>, header<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span></code></pre></div><p>I believe that 50% of writing good tests is actually writing easily testable code.
So lets restructure the code above a bit.</p>
<p>The first thing I want to know, when trying to understand a new spark script,
is what are the inputs and what is the output, so I like to keep that information
right at the top.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>TARGET <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;./full_sales.csv&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>SOURCES <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;sales&#34;</span>: <span style="color:#e6db74">&#34;./sales.csv&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;users&#34;</span>: <span style="color:#e6db74">&#34;./user.csv&#34;</span>,
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>Next, although getting the correct read and write parameters can be a bit tricky
at first, its probably not where the bulk of the complexity of your spark job
will be. For this reason I like to have a <code>transform(...)</code> function exclusively
responsible for taking dataframes in and returning a dataframe out, without
worrying  about the format on disk.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">transform</span>(sales: DataFrame, users: DataFrame) <span style="color:#f92672">-&gt;</span> DataFrame:
</span></span><span style="display:flex;"><span>    df <span style="color:#f92672">=</span> sales<span style="color:#f92672">.</span>join(users, on<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;user_id&#34;</span>], how<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;left&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> df
</span></span></code></pre></div><p>In this case our transform function is quite trivial but we&rsquo;ll discuss later on
why I believe its still worth testing.</p>
<p>The full restructured script would look something like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>TARGET <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;./full_sales.csv&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>SOURCES <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;sales&#34;</span>: <span style="color:#e6db74">&#34;./sales.csv&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;users&#34;</span>: <span style="color:#e6db74">&#34;./user.csv&#34;</span>,
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">main</span>(spark, sources, target):
</span></span><span style="display:flex;"><span>    inputs <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>        k: spark<span style="color:#f92672">.</span>read<span style="color:#f92672">.</span>csv(v, sep<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">&#34;</span>, header<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> k, v <span style="color:#f92672">in</span> sources<span style="color:#f92672">.</span>items()
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    df <span style="color:#f92672">=</span> transform(<span style="color:#f92672">**</span>inputs)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    df<span style="color:#f92672">.</span>write<span style="color:#f92672">.</span>mode(<span style="color:#e6db74">&#34;overwrite&#34;</span>)<span style="color:#f92672">.</span>csv(target, sep<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">&#34;</span>, header<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">transform</span>(sales: DataFrame, users: DataFrame) <span style="color:#f92672">-&gt;</span> DataFrame:
</span></span><span style="display:flex;"><span>    df <span style="color:#f92672">=</span> sales<span style="color:#f92672">.</span>join(users, on<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;user_id&#34;</span>], how<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;left&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> df
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;__main__&#34;</span>:
</span></span><span style="display:flex;"><span>    spark <span style="color:#f92672">=</span> SparkSession<span style="color:#f92672">.</span>builder<span style="color:#f92672">.</span>master(<span style="color:#e6db74">&#34;local&#34;</span>)<span style="color:#f92672">.</span>getOrCreate()
</span></span><span style="display:flex;"><span>    main(spark, SOURCES, TARGET)
</span></span></code></pre></div><p>With this we can easily swap inputs and outputs, and we can also test the
transformation without having to worry about file formats.</p>
<h2 id="testing-in-this-pattern">Testing in this pattern</h2>
<p>So now that we refactored the code, how can we write a unit test for it? We&rsquo;ll
rely on the idea that we can create a dataframe from a tuple of tuples. Like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>data <span style="color:#f92672">=</span> (
</span></span><span style="display:flex;"><span>    (<span style="color:#e6db74">&#34;transaction1&#34;</span>, <span style="color:#ae81ff">10.0</span>, <span style="color:#e6db74">&#34;user1&#34;</span>),
</span></span><span style="display:flex;"><span>    (<span style="color:#e6db74">&#34;transaction2&#34;</span>, <span style="color:#ae81ff">20.0</span>, <span style="color:#e6db74">&#34;user2&#34;</span>),
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>schema <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;transaction_id: string,  price: double, user_id: string&#34;</span>
</span></span><span style="display:flex;"><span>df <span style="color:#f92672">=</span> spark<span style="color:#f92672">.</span>createDataFrame(data, schema<span style="color:#f92672">=</span>schema)
</span></span></code></pre></div><p>Now that we can create example dataframes, testing pyspark becomes just like testing
any other function! We create a sample input and an expected output and compare the
actual output with what we expect.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">test_transform</span>():
</span></span><span style="display:flex;"><span>    spark <span style="color:#f92672">=</span> SparkSession<span style="color:#f92672">.</span>builder<span style="color:#f92672">.</span>master(<span style="color:#e6db74">&#34;local&#34;</span>)<span style="color:#f92672">.</span>getOrCreate()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    sales <span style="color:#f92672">=</span> (
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;transaction1&#34;</span>, <span style="color:#ae81ff">10.0</span>, <span style="color:#e6db74">&#34;user1&#34;</span>),
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;transaction2&#34;</span>, <span style="color:#ae81ff">20.0</span>, <span style="color:#e6db74">&#34;user2&#34;</span>),
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    schema <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;transaction_id: string,  price: double, user_id: string&#34;</span>
</span></span><span style="display:flex;"><span>    sales <span style="color:#f92672">=</span> spark<span style="color:#f92672">.</span>createDataFrame(sales, schema<span style="color:#f92672">=</span>schema)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    users <span style="color:#f92672">=</span> (
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;user1&#34;</span>, <span style="color:#e6db74">&#34;Jonh&#34;</span>),
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;user2&#34;</span>, <span style="color:#e6db74">&#34;Rebeca&#34;</span>),
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    schema <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;user_id: string, name: string&#34;</span>
</span></span><span style="display:flex;"><span>    users <span style="color:#f92672">=</span> spark<span style="color:#f92672">.</span>createDataFrame(users, schema<span style="color:#f92672">=</span>schema)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    expected <span style="color:#f92672">=</span> (
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;transaction1&#34;</span>, <span style="color:#ae81ff">10.0</span>, <span style="color:#e6db74">&#34;user1&#34;</span>, <span style="color:#e6db74">&#34;John&#34;</span>),
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;transaction2&#34;</span>, <span style="color:#ae81ff">20.0</span>, <span style="color:#e6db74">&#34;user2&#34;</span>, <span style="color:#e6db74">&#34;Rebeca&#34;</span>),
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    schema <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;transaction_id: string, name: string&#34;</span>
</span></span><span style="display:flex;"><span>    expected <span style="color:#f92672">=</span> spark<span style="color:#f92672">.</span>createDataFrame(expected, schema<span style="color:#f92672">=</span>schema)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    got <span style="color:#f92672">=</span> transform(sales, users)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    assert_dataframe_equal(got, expected)
</span></span></code></pre></div><p>I hid a bit of dark magic inside <code>assert_dataframe_equal(...)</code> to make it
simpler to read. We will come back to this after we cover a few other points.</p>
<h1 id="things-to-keep-in-mind">Things to keep in mind</h1>
<p>From the description above I make it sound like testing pyspark is just like testing
python. But if you&rsquo;ve tried this before you&rsquo;ve probably realized that there are a few
caveats that you should pay attention to. Here are a couple of them.</p>
<h2 id="shuffle-partitions">Shuffle partitions</h2>
<p>If you&rsquo;ve been using spark for a while, you&rsquo;ll know that every time that you
do a <code>join</code> or a <code>group by</code> spark has to do a &ldquo;shuffle&rdquo; to position data from
the same Keys in the same machine. This is the famous &ldquo;reduce&rdquo; in the <code>Map Reduce</code>
paradigm. If you have a very big dataset and use the default 200
for the number of shuffle partitions each of the 200 pieces will still be very big
and your data will spill to disk which will make your job much slower. So usually
when you want to change shuffle partitions you want to make it bigger.</p>
<p>In the case of unit-tests its the oposite your dataset has just a few lines. If you
use the default most of the partitions will be empty, but they will be created and
require coordination from the scheduler. This makes our test run slower than it has to.
To avoid this, we set the shuffle partitions to 1 for the tests.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>spark<span style="color:#f92672">.</span>conf<span style="color:#f92672">.</span>set(<span style="color:#e6db74">&#34;spark.sql.shuffle.partitions&#34;</span>, <span style="color:#ae81ff">1</span>)
</span></span></code></pre></div><p>Even with this optimization, starting up a spark session still takes a few seconds,
so your pyspark tests will take a bit longer than you normal python tests. :/</p>
<h2 id="minimal-test-schema">Minimal test schema</h2>
<p>Another point of friction is that datalake tables tend to be very wide to avoid joins.
Writing out all of the columns for a test case can be very demotivating.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># sales schema</span>
</span></span><span style="display:flex;"><span>transaction_id	sku	price	user_id   date  country
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># users schema</span>
</span></span><span style="display:flex;"><span>user_id	name  e-mail  favorite-color  nick-name
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># output schema</span>
</span></span><span style="display:flex;"><span>transaction_id	sku	price	user_id   date  country name  e-mail  favorite-color  nick-name
</span></span></code></pre></div><p>In our test case actually uses the <code>user_id</code> column so we can write our tests
using only that column. Well, actually we need an extra column from each table
to be able to verify the join.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># sales schema</span>
</span></span><span style="display:flex;"><span>transaction_id	user_id
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># users schema</span>
</span></span><span style="display:flex;"><span>user_id	name
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># output schema</span>
</span></span><span style="display:flex;"><span>transaction_id	user_id name
</span></span></code></pre></div><p>For the output schema it can be restricted to only the columns you want to check.
So if in another test we want to check if the price was correctly converted
into dollars, then we no longer need to check the user_id.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># output schema</span>
</span></span><span style="display:flex;"><span>transaction_id	usd_pricer
</span></span></code></pre></div><h2 id="compare-function">Compare function</h2>
<p>Comparing spark dataframes can be a bit tricky. There are many ways two
dataframes can be different. They can have different columns, different
column types or different column and they can differ in value. The very
least that we need is that any one of these errors triggers an assert failure.
We can do this by ordering the dataframes then collecting them.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">assert_dataframes_equal</span>(df, df_ref):
</span></span><span style="display:flex;"><span>    cols <span style="color:#f92672">=</span> df_ref<span style="color:#f92672">.</span>columns
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># order datasets</span>
</span></span><span style="display:flex;"><span>    actual_df <span style="color:#f92672">=</span> df<span style="color:#f92672">.</span>select(cols)<span style="color:#f92672">.</span>orderBy(cols)
</span></span><span style="display:flex;"><span>    expected_df <span style="color:#f92672">=</span> df_ref<span style="color:#f92672">.</span>orderBy(cols)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> actual_df<span style="color:#f92672">.</span>collect() <span style="color:#f92672">!=</span> expected_df<span style="color:#f92672">.</span>collect():
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">assert</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">assert</span> <span style="color:#66d9ef">True</span>
</span></span></code></pre></div><p>This function will serve its purpose, but when the assertion fails
you will be left with some very unhelpful error messages.
This is very bad for testing because if you need to push a new feature
and you can&rsquo;t figure out what is failing in the tests, that can be very
frustrating.</p>
<p>I have slowly built up an assert function that gives help full
messages upon failures. The <a href="https://github.com/glazari/snippets/blob/master/testing-pyspark/pyspark_test.py">full code is on github</a>. I intend on
creating a small library with it when I have the time. Here is
rough sketch of the extra steps for better messages.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">assert_dataframes_equal</span>(df, df_ref, order_cols<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>    cols <span style="color:#f92672">=</span> df_ref<span style="color:#f92672">.</span>columns
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    missing_cols <span style="color:#f92672">=</span> set(cols) <span style="color:#f92672">-</span> set(df<span style="color:#f92672">.</span>columns)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> missing_cols:
</span></span><span style="display:flex;"><span>        print_diff_cols(df_ref, df)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">assert</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    assert_column_types(df_ref, df)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> order_cols:
</span></span><span style="display:flex;"><span>        order_cols <span style="color:#f92672">=</span> cols
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># order datasets</span>
</span></span><span style="display:flex;"><span>    actual_df <span style="color:#f92672">=</span> df<span style="color:#f92672">.</span>select(cols)<span style="color:#f92672">.</span>orderBy(order_cols)
</span></span><span style="display:flex;"><span>    expected_df <span style="color:#f92672">=</span> df_ref<span style="color:#f92672">.</span>orderBy(order_cols)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    actual <span style="color:#f92672">=</span> actual_df<span style="color:#f92672">.</span>collect()
</span></span><span style="display:flex;"><span>    expected <span style="color:#f92672">=</span> expected_df<span style="color:#f92672">.</span>collect()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> actual <span style="color:#f92672">!=</span> expected:
</span></span><span style="display:flex;"><span>        print_full_diff(expected_df, actual_df)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">assert</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">assert</span> <span style="color:#66d9ef">True</span>
</span></span></code></pre></div><h1 id="excuses-for-not-writing-pyspark-tests">Excuses for not writing pyspark tests</h1>
<p>Finally, lets go over a few excuses that might keep you from testing your code
and try to debunk them real quick.</p>
<h2 id="i-prefer-sql-pyspark-is-too-verbose">&ldquo;I prefer sql, pyspark is too verbose&rdquo;</h2>
<p>For those of you who prefer to write in sql, here is a way to still do it:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">transform</span>(sales: DataFrame, users: DataFrame) <span style="color:#f92672">-&gt;</span> DataFrame:
</span></span><span style="display:flex;"><span>    sales<span style="color:#f92672">.</span>createOrReplaceTempView(<span style="color:#e6db74">&#34;sales&#34;</span>)
</span></span><span style="display:flex;"><span>    users<span style="color:#f92672">.</span>createOrReplaceTempView(<span style="color:#e6db74">&#34;users&#34;</span>)
</span></span><span style="display:flex;"><span>    query <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">      select * from sales s left join users on s.user_id = u.users_id
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    df <span style="color:#f92672">=</span> spark<span style="color:#f92672">.</span>sql(query)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> df
</span></span></code></pre></div><p>I realize the irony that this is more verbose than the pure pyspark version.
But hey, its in sql and now you can test it! :)</p>
<h2 id="its-just-a-join">&ldquo;Its just a join&rdquo;</h2>
<p>I&rsquo;ve heard this a lot: &ldquo;its too simple to test&rdquo;. Even in the simplest cases
you&rsquo;d be surprised at how many times there can be a bug. But a better argument
for testing the simple cases is that they inevitably will become more complex
over time, and when it does, your test is already setup for you to extend it.</p>
<p>In our sales example we will soon receive the request to convert different
currencies into dollars or discount different country taxes.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">transform</span>(sales, users, exchange_rates, tax_rates):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># exchange rates</span>
</span></span><span style="display:flex;"><span>    df <span style="color:#f92672">=</span> sales<span style="color:#f92672">.</span>join(exchange_rates, on<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;date&#39;</span>, <span style="color:#e6db74">&#39;currency&#39;</span>], how<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;left&#34;</span>)
</span></span><span style="display:flex;"><span>    df <span style="color:#f92672">=</span> df<span style="color:#f92672">.</span>withColumn(<span style="color:#e6db74">&#34;usd_price&#34;</span>, df<span style="color:#f92672">.</span>price <span style="color:#f92672">*</span> df<span style="color:#f92672">.</span>exchange_rate)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># taxes</span>
</span></span><span style="display:flex;"><span>    df <span style="color:#f92672">=</span> df<span style="color:#f92672">.</span>join(tax_rates, on<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;country&#39;</span>], how<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;left&#34;</span>)
</span></span><span style="display:flex;"><span>    df <span style="color:#f92672">=</span> df<span style="color:#f92672">.</span>withColumn(<span style="color:#e6db74">&#34;taxes&#34;</span>, df<span style="color:#f92672">.</span>usd_price <span style="color:#f92672">*</span> df<span style="color:#f92672">.</span>tax_rate)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    df <span style="color:#f92672">=</span> sales<span style="color:#f92672">.</span>join(users, on<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;user_id&#34;</span>], how<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;left&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> df
</span></span></code></pre></div><h1 id="thats-it-happy-testing-">That&rsquo;s it, Happy testing !!!</h1>
<p>Full code at: <a href="https://github.com/glazari/snippets/tree/master/testing-pyspark">testing-pyspark</a></p>




      
        <div class="blog-tags">
          
            <a href="https://blog.guilhermelazari.dev//tags/spark/">spark</a
            >&nbsp;
          
            <a href="https://blog.guilhermelazari.dev//tags/testing/">testing</a
            >&nbsp;
          
            <a href="https://blog.guilhermelazari.dev//tags/python/">python</a
            >&nbsp;
          
        </div>
      
    </article>
    
    
      

    
  </div>

    <footer>
  
  <div>
    
      <a href="https://github.com/glazari" name="github"
        ><em class="fab fa-github"></em
      ></a>
    
      <a href="https://www.linkedin.com/in/guilherme-l%c3%a1zari-costa-e-silva-7a074b59/" name="linkedin"
        ><em class="fab fa-linkedin"></em
      ></a>
    
  </div>


  <div class="container">
    <p class="credits copyright">
      <a href="https://blog.guilhermelazari.dev/about">Guilherme de LÃ¡zari</a>
      &nbsp;&copy;
      2021
      
        &nbsp;/&nbsp;
        <a href="https://blog.guilhermelazari.dev/">Guilherme LÃ¡zari</a>
      
      &nbsp;&ndash;&nbsp;
      <em class="fas fa-moon" id="dark-mode-toggle"></em>
    </p>

    <p class="credits theme-by">
      Powered By <a href="https://gohugo.io">Hugo</a>&nbsp;
      Theme
      <a href="https://github.com/matsuyoshi30/harbor">Harbor</a>
    </p>
  </div>
</footer>

  </body>
</html>
